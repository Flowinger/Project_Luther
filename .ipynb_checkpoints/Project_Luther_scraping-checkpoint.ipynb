{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from dateutil.parser import parse\n",
    "import re\n",
    "import numpy as np\n",
    "url = 'http://www.transfermarkt.com/manuel-neuer/profil/spieler/17259'\n",
    "\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fake_useragent import UserAgent\n",
    "ua = UserAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'User-agent': 'Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/31.0.1650.16 Safari/537.36'}\n"
     ]
    }
   ],
   "source": [
    "user_agent = {'User-agent': ua.random}\n",
    "print(user_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response = requests.get(url, headers = user_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "page = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(page, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for scraping features from each player site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "infos = ['Date of birth:','Place of birth:','Age:','Height:','Nationality:','Position:','Foot:',\"\"\"Player's agent:\"\"\",\\\n",
    "         'Current club:','In the team since:','Contract until:','Date of last contract extension:','Outfitter:']\n",
    "def player_name(soup):\n",
    "    #return soup.find('h1').text\n",
    "    #print(soup)\n",
    "    return soup.find_all('title')[0].text.split(' -')[0]\n",
    "\n",
    "def player_values(soup, field_name):\n",
    "    obj = soup.find(text=re.compile(field_name))\n",
    "    if not obj: \n",
    "        return None\n",
    "    # this works for most of the values\n",
    "    next = obj.findNext()\n",
    "    if next:\n",
    "        return next.text.strip()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def market_values(soup):\n",
    "    values = []\n",
    "    obj = soup.find_all(class_=re.compile('left-td'))\n",
    "    for x in obj:\n",
    "        value = x.findNextSibling().text.strip()\n",
    "        \n",
    "        if '\\n' in value:\n",
    "            value = value.split('\\n')\n",
    "            value1, value2 = value\n",
    "            value1 = value1.replace('\\t','')\n",
    "            values.append(change_euro_value(value1))\n",
    "            values.append(change_euro_value(value2))\n",
    "        else:\n",
    "            values.append(change_euro_value(value))\n",
    "    return values\n",
    "        \n",
    "def transfer_proceeds(soup):\n",
    "    transfer_proceeds = soup.find('tfoot').text.strip().split('\\n')[2]\n",
    "    return change_euro_value(transfer_proceeds)\n",
    "\n",
    "def performance(soup):\n",
    "    values = []\n",
    "    for i in soup.find_all('table')[2].find_all('td')[2:7]:\n",
    "        values.append(i.text.replace('.',''))\n",
    "    return values\n",
    "\n",
    "def wc_winner(soup):\n",
    "    if soup.find('img', {'alt': 'World Cup winner'}) in soup.find_all('img'):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def cl_winner(soup):\n",
    "    if soup.find('img', {'alt': 'Champions League winner'}) in soup.find_all('img'):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def player_international_exp(soup):\n",
    "    try:\n",
    "        return soup.find(text=re.compile('International caps/goals:')).findNext().text.split('/')[0]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def scrape_int_exp(soup):\n",
    "    inf = []\n",
    "    int_exp = player_international_exp(soup)\n",
    "    name = player_name(soup)\n",
    "    inf.append(name)\n",
    "    inf.append(int_exp)\n",
    "    return tuple(inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_list = []\n",
    "lks = []\n",
    "while len(player_urls) > 0:\n",
    "    for i in player_urls[0]:\n",
    "        player_soup = get_soup(i)\n",
    "        new_list.append(scrape_int_exp(player_soup))\n",
    "    lks.append(player_urls.pop(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def change_euro_value(value):\n",
    "    value = value.replace(' Th. €', '000')\n",
    "    value = value.replace(' Mill. €','0000')\n",
    "    value = value.replace(',','')\n",
    "    value = value.replace('\\t','')\n",
    "    value = value.replace('\\n','')\n",
    "    return value\n",
    "\n",
    "def get_soup(url):\n",
    "    ua = UserAgent()\n",
    "    user_agent = {'User-agent': ua.random}\n",
    "    response = requests.get(url, headers = user_agent)\n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    return soup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_player(soup):\n",
    "    player_info = []\n",
    "    player_columns = ['name','date_of_birth','place_of_birth','age','height','nationality','position', \\\n",
    "                 'foot','agency','current_club','in_team_since','contract_until', \\\n",
    "                 'date_last_contract_extension','outfitter', \\\n",
    "                  'current_market_value','date_last_market_value_change','highest_market_value', \\\n",
    "                  'date_highest_market_value', 'total_transfer_proceeds','apps_season','goals_season',\\\n",
    "                 'assists_season','minute_per_goal','played_minutes','wc_winner','cl_winner']\n",
    "    infos = ['Date of birth:','Place of birth:','Age:','Height:','Nationality:','Position:','Foot:',\"\"\"Player's agent:\"\"\",\\\n",
    "         'Current club:','In the team since:','Contract until:','Date of last contract extension:','Outfitter:']\n",
    "    # add player name\n",
    "    player_info.append(player_name(soup))\n",
    "    \n",
    "    # add standard player values\n",
    "    for value in infos:\n",
    "        player_info.append(player_values(soup, value))\n",
    "        \n",
    "    # add market value infos\n",
    "    for value in market_values(soup):\n",
    "        player_info.append(value)\n",
    "        \n",
    "    # add transfer proceeds\n",
    "    player_info.append(transfer_proceeds(soup))\n",
    "    \n",
    "    # add performance data\n",
    "    for value in performance(soup):\n",
    "        player_info.append(value)\n",
    "        \n",
    "    # add if World Cup or Champions League winner (0/1)\n",
    "    player_info.append(wc_winner(soup))\n",
    "    player_info.append(cl_winner(soup))\n",
    "    \n",
    "    # create player dict and return Series\n",
    "    #player = dict(zip(player_columns, player_info))\n",
    "    #return pd.Series(player)\n",
    "    \n",
    "    return tuple(player_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all links for scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_club_urls(soup):\n",
    "    urls = []\n",
    "    for url in soup.find_all('tr', {'class': 'odd'}):\n",
    "        if url.find('a')['href'] != '#':\n",
    "            urls.append(url.find('a')['href'])\n",
    "    for url2 in soup.find_all('tr', {'class': 'even'}):\n",
    "        if url2.find('a')['href'] != '#':\n",
    "            urls.append(url2.find('a')['href'])\n",
    "    return urls\n",
    "\n",
    "def get_player_urls(soup):\n",
    "    player_urls = []\n",
    "    for i in soup.find_all('a', {'class': 'spielprofil_tooltip'}):\n",
    "        player_url = 'http://www.transfermarkt.com' + i['href']\n",
    "        if player_url not in player_urls:\n",
    "            player_urls.append(player_url)\n",
    "    return player_urls\n",
    "\n",
    "def scrape_club(soup):\n",
    "    for i in get_player_urls(club):\n",
    "        #print(i)\n",
    "        soup = get_soup(i)\n",
    "        player = create_player(soup)\n",
    "        df = df.append(player, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping links of all clubs.\n",
    "First 2 leagues of each country: Germany, UK, France, Spain, Italy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "leagues = ['L1','L2','GB1','GB2','FR1','FR2','ES1','ES2','IT1','IT2']\n",
    "league_urls = ['http://www.transfermarkt.com/jumplist/startseite/wettbewerb/'+ x for x in leagues]\n",
    "club_urls = [get_club_urls(get_soup(league)) for league in league_urls]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#club_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "player_urls = [get_player_urls(get_soup('http://www.transfermarkt.com/'+club)) for clubs in club_urls for club in clubs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape team by team and save players in list 'players'. Pop the scraped teams from the link list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#players = []\n",
    "#scraped_links = []\n",
    "#len(player_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all 204 teams were scraped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "while len(player_urls) > 150:\n",
    "    for player in player_urls[0]:\n",
    "        player_soup = get_soup(player)\n",
    "        players.append(create_player(player_soup))\n",
    "    scraped_links.append(player_urls.pop(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#len(players), len(player_urls), len(scraped_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "player_columns = ['name','date_of_birth','place_of_birth','age','height','nationality','position', \\\n",
    "                 'foot','agency','current_club','in_team_since','contract_until', \\\n",
    "                 'date_last_contract_extension','outfitter', \\\n",
    "                  'current_market_value','date_last_market_value_change','highest_market_value', \\\n",
    "                  'date_highest_market_value', 'total_transfer_proceeds','apps_season','goals_season',\\\n",
    "                 'assists_season','minute_per_goal','played_minutes','wc_winner','cl_winner']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df['contract_until'] = df['contract_until'].map(lambda x: '30.06.2019' if x == '31.06.2019' else x)\n",
    "#df['minute_per_goal'] = df['minute_per_goal'].fillna(0)\n",
    "#df['days_to_contract_expiry'] = df['days_to_contract_expiry'].map(lambda x: x / np.timedelta64(1, 'D'))\n",
    "#df['contract_until'] = df['contract_until'].map(lambda x: x.strip('T00:00:00.000000000'))\n",
    "#df[df['total_transfer_proceeds'] != int()]\n",
    "#df[df['foot'] == np.isnan]\n",
    "#df.columns\n",
    "#df['contract_until'] = df['contract_until'].apply(pd.to_datetime, format='%Y-%m-%d', errors='ignore')\n",
    "#df.iloc[:,11:12][df.iloc[:,11:12].isnull().any(axis=1)] # show NaN values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save and load pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import datetime\n",
    "\n",
    "def save_df(df):\n",
    "    day = datetime.datetime.now().day\n",
    "    hour = datetime.datetime.now().hour\n",
    "    minute = datetime.datetime.now().minute\n",
    "    with open('soccer_scrape_'+str(day)+'-'+str(hour)+'h-'+str(minute)+'m.pkl', 'wb') as picklefile:\n",
    "        pickle.dump(df, picklefile)\n",
    "def load_df(filename):\n",
    "    with open(filename, 'rb') as picklefile: \n",
    "        last_version = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df = pd.read_csv('0423_2014pm.csv')\n",
    "\n",
    "#df.to_csv('0423_2014pm.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
